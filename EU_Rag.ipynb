{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3766bd3-3cab-487d-9c14-7196c2056a49",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d76d3f-6009-4193-baa6-94527ba511b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "voyage_api_key = \"\"  # Replace with your API key\n",
    "gemini_api_key = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cbbf9-e5a1-4904-8e2a-95a2c14c126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install voyageai\n",
    "!{sys.executable} -m pip install -q -U google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2148f51-0846-4880-bce7-866c8ea31bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Replace `mysecretpassword` with the password you set in the Docker command\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",       # Default database name\n",
    "    user=\"postgres\",         # Default user\n",
    "    password=\"mysecretpassword\",  # Password from the Docker setup\n",
    "    host=\"localhost\",        # PostgreSQL is hosted locally\n",
    "    port=\"5432\"              # Default PostgreSQL port\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    print(\"Connected to PostgreSQL!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e957d62-2670-4319-b10c-ea7ee5f3f024",
   "metadata": {},
   "source": [
    "# Split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a41f76-a4c8-451d-8bf4-b61c28e6b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 654850 characters from the file.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the EU AI Act text\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage\n",
    "file_path = \"act.md\"  # Update with the path to your file\n",
    "eu_ai_act_text = load_text(file_path)\n",
    "print(f\"Loaded {len(eu_ai_act_text)} characters from the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9191d2c-555b-4d58-94b4-dc9adc8d2471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 525 chunks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 2: Split text into manageable chunks\n",
    "def split_text(text, max_words=200):\n",
    "    # Split text into paragraphs\n",
    "    paragraphs = re.split(r'\\n+', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split()\n",
    "        if current_word_count + len(words) > max_words:\n",
    "            # Start a new chunk if the limit is exceeded\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_word_count = 0\n",
    "        current_chunk.append(paragraph)\n",
    "        current_word_count += len(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))  # Add the last chunk\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "chunks = split_text(eu_ai_act_text)\n",
    "print(f\"Split into {len(chunks)} chunks.\")\n",
    "print(chunks[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519bc22-6730-47e1-9032-84ffa1968bec",
   "metadata": {},
   "source": [
    "# Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75f1d517-d87a-472e-a279-ebc9796b6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(chunk, model=\"voyage-3\"):\n",
    "    documents_embeddings = vo.embed(\n",
    "        chunk, model=\"voyage-3\", input_type=\"document\"\n",
    "        ).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5382dcb8-ad83-41e4-9477-1f1241235d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 525 chunks with embeddings.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def prepare_data(chunks, api_key):\n",
    "    data = []\n",
    "    num_chunk = 0\n",
    "    for chunk in chunks:\n",
    "        num_chunk += 1\n",
    "        try:\n",
    "            embedding = generate_embedding(chunk)\n",
    "            data.append({\"text\": chunk, \"embedding\": embedding})\n",
    "        except:\n",
    "          print(\"Reached chunk: \" + num_chunk)\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = prepare_data(chunks, api_key)\n",
    "print(f\"Prepared {len(data)} chunks with embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62e092f5-80d9-4609-9aa3-16c6ac9f58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c5301-f043-4720-88cd-d76c86c14e31",
   "metadata": {},
   "source": [
    "# Insert into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e14636b1-f65e-4758-bdf0-14401b8d2091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into the database successfully.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "\n",
    "def insert_into_db(data, conn_params):\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    INSERT INTO embeddings (text, embedding)\n",
    "    VALUES (%s, %s)\n",
    "    \"\"\"\n",
    "    records = [(item[\"text\"], item[\"embedding\"]) for item in data]\n",
    "\n",
    "    execute_batch(cur, query, records)\n",
    "    conn.commit()\n",
    "\n",
    "db_params = {\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"mysecretpassword\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432\n",
    "}\n",
    "insert_into_db(data, db_params)\n",
    "print(\"Data inserted into the database successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1047d5d-7f79-49bf-8a81-31711634b7bb",
   "metadata": {},
   "source": [
    "# Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "972846fe-e123-4bb9-b692-90562ffeed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embeddings(query_vector):\n",
    "    # Perform similarity search\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"SELECT text FROM embeddings ORDER BY embedding <=> %s LIMIT 5\", (query_vector,))\n",
    "    results = cur.fetchall()\n",
    "    return [result[0] for result in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b73fecc2-3c90-44c0-8e79-98288df4dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Query Gemini Flash using the genai package\n",
    "def query_gemini_flash(context, query, api_key=gemini_api_key):\n",
    "    # Initialize the generative model\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    # Prepare the prompt combining context and query\n",
    "    prompt = f\"Based on the information: {context}\\n\\n Generate a response of: {query}\"\n",
    "    print(\"Prompt: \" + prompt)\n",
    "\n",
    "    # Generate the response from the model\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return response.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2a87609-b207-4890-9f33-361ecdd0e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the EU AI Act Chatbot!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  What information should the EU declaration of conformity contain?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Based on the information:          \n",
      "\n",
      " Generate a response of: What information should the EU declaration of conformity contain?\n",
      "\n",
      "Answer: Please provide the information you'd like me to base my response on. I need the text or a link to the information about EU Declarations of Conformity to accurately tell you what they should contain.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Welcome to the EU AI Act Chatbot!\")\n",
    "    while True:\n",
    "        question = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate embedding for the question\n",
    "        question_vector = generate_embedding(question)\n",
    "        \n",
    "        # Retrieve relevant sections\n",
    "        results = query_embeddings(question_vector)\n",
    "        context = \" \".join([row[0] for row in results])\n",
    "        \n",
    "        # Generate and display response\n",
    "        answer = query_gemini_flash(context, question)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5a664-9998-467d-9115-7a02586a4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "What information should the EU declaration of conformity contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1097a1b-8d51-4421-b3ca-ee1d7c590856",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "        \"dbname\": \"postgres\",\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"mysecretpassword\",\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 5432\n",
    "    }\n",
    "\n",
    "    # API keys\n",
    "    gemini_api_key = \"AIzaSyAvUbnNinMR1i5DwOZw1BiQRr2MZZvlRvY\"\n",
    "\n",
    "    # Start chatbot\n",
    "    chatbot_interface(db_params, lambda query: openai_embedding_model(query, openai_api_key), gemini_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
